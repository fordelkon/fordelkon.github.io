<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深度学习中常用的概率分布总结 | DL Kong</title><meta name=keywords content="Probability"><meta name=description content="在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集$ \mathcal{D}=\lbrace(\mathbf{x}_{i}, y_{i})\rbrace_{i=1}^{N} $或$ \mathcal{D}=\lbrace\mathbf{x}_{i}\rbrace_{i=1}^{N} $，主要区分于有标签和无标签的情况，$ z $或$ \mathbf{z}$ 主要表示深度学习模型未经处理的输出。"><meta name=author content="DL Kong"><link rel=canonical href=https://fordelkon.github.io/teaching/probdis/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.c7a0847263ef81d155d535eb3508757a49b1a3680807f60ed1f64e9d4a686070.css integrity="sha256-x6CEcmPvgdFV1TXrNQh1ekmxo2gIB/YO0fZOnUpoYHA=" rel="preload stylesheet" as=style><link rel=icon href=https://fordelkon.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fordelkon.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fordelkon.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fordelkon.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fordelkon.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fordelkon.github.io/teaching/probdis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><link rel=stylesheet href=/css/extended/xcode.css media="(prefers-color-scheme: light)"><link rel=stylesheet href=/css/extended/monokai.css media="(prefers-color-scheme: dark)"><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT")}</script><meta property="og:url" content="https://fordelkon.github.io/teaching/probdis/"><meta property="og:site_name" content="DL Kong"><meta property="og:title" content="深度学习中常用的概率分布总结"><meta property="og:description" content="在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集$ \mathcal{D}=\lbrace(\mathbf{x}_{i}, y_{i})\rbrace_{i=1}^{N} $或$ \mathcal{D}=\lbrace\mathbf{x}_{i}\rbrace_{i=1}^{N} $，主要区分于有标签和无标签的情况，$ z $或$ \mathbf{z}$ 主要表示深度学习模型未经处理的输出。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="teaching"><meta property="article:published_time" content="2025-08-24T18:13:33+09:00"><meta property="article:modified_time" content="2025-08-24T18:13:33+09:00"><meta property="article:tag" content="Probability"><meta property="og:image" content="https://fordelkon.github.io/img/merge.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fordelkon.github.io/img/merge.png"><meta name=twitter:title content="深度学习中常用的概率分布总结"><meta name=twitter:description content="在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集$ \mathcal{D}=\lbrace(\mathbf{x}_{i}, y_{i})\rbrace_{i=1}^{N} $或$ \mathcal{D}=\lbrace\mathbf{x}_{i}\rbrace_{i=1}^{N} $，主要区分于有标签和无标签的情况，$ z $或$ \mathbf{z}$ 主要表示深度学习模型未经处理的输出。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Teaching","item":"https://fordelkon.github.io/teaching/"},{"@type":"ListItem","position":2,"name":"深度学习中常用的概率分布总结","item":"https://fordelkon.github.io/teaching/probdis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深度学习中常用的概率分布总结","name":"深度学习中常用的概率分布总结","description":"在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集$ \\mathcal{D}=\\lbrace(\\mathbf{x}_{i}, y_{i})\\rbrace_{i=1}^{N} $或$ \\mathcal{D}=\\lbrace\\mathbf{x}_{i}\\rbrace_{i=1}^{N} $，主要区分于有标签和无标签的情况，$ z $或$ \\mathbf{z}$ 主要表示深度学习模型未经处理的输出。\n","keywords":["Probability"],"articleBody":"在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集$ \\mathcal{D}=\\lbrace(\\mathbf{x}_{i}, y_{i})\\rbrace_{i=1}^{N} $或$ \\mathcal{D}=\\lbrace\\mathbf{x}_{i}\\rbrace_{i=1}^{N} $，主要区分于有标签和无标签的情况，$ z $或$ \\mathbf{z}$ 主要表示深度学习模型未经处理的输出。\n常用概率分布 连续随机变量分布 正态分布（单变量） $$ y\\sim\\mathcal{N}(z, \\sigma^2)\\tag{1-1} $$ $$ p(y\\mid z) = \\frac{1}{\\sqrt{ 2\\pi \\sigma^2 }}\\exp\\Big[ -\\frac{(y-z)^2}{2\\sigma^2}\\Big]\\tag{1-2} $$ 单变量正态分布概率密度函数曲线如图所示：\n上述条件概率经过$ \\log $化取负之后化简其对应的损失函数\n$$ \\begin{align} \\arg\\min -\\log p(y\\mid z) \u0026= \\arg\\min \\Big[\\frac{1}{2}\\log [ 2\\pi \\sigma^2] + \\frac{(y - z)^2}{2\\sigma^2}\\Big] \\\\ \u0026= \\arg\\min [(y - z)^2] \\quad (\\sigma\\: is \\: constant) \\end{align}\\tag{1-3} $$ 正态分布（多变量） $$ \\mathbf{y}\\sim \\mathcal{N}(\\mathbf{z}, \\Sigma)\\tag{2-1} $$ $$ p(\\mathbf{y}\\mid \\mathbf{z}) = \\frac{1}{(2\\pi)^{K/2}|\\Sigma|^{1/2}}\\exp\\Big[ -\\frac{(\\mathbf{y}-\\mathbf{z})^{\\mathrm{T}}\\Sigma^{-1}(\\mathbf{y}-\\mathbf{z})}{2}\\Big]\\tag{2-2} $$ 二维正态分布概率密度等高线如图所示\n同理，上述条件概率经过$ \\log $化取负之后化简得到其对应的损失函数\n$$ \\begin{align} \\arg\\min -\\log p(\\mathbf{y}\\mid \\mathbf{z}) \u0026= \\arg\\min \\left[\\frac{K}{2}\\log[2\\pi] + \\frac{1}{2}\\log |\\Sigma| + \\frac{1}{2}(\\mathbf{y} - \\mathbf{z})^\\mathrm{T}\\Sigma^{-1}(\\mathbf{y} - \\mathbf{z})\\right] \\\\ \u0026= \\arg\\min \\left[\\frac{K}{2}\\log[2\\pi \\sigma^2] + \\frac{1}{2\\sigma^2}\\lvert \\lvert \\mathbf{y} - \\mathbf{z} \\rvert \\rvert^2 \\right]\\quad (\\Sigma = \\sigma^2\\mathbf{I}) \\\\ \u0026= \\arg\\min \\left[\\lvert \\lvert \\mathbf{y} - \\mathbf{z} \\rvert \\rvert^2 \\right]\\quad (\\sigma\\:is\\: constant) \\end{align}\\tag{2-3} $$ 混合高斯分布 $$ y\\sim \\mathrm{GMM}(\\mathbf{z},\\boldsymbol{\\sigma}^2)\\quad\\mathbf{z}=[z_{1}, \\dots, z_{K}]^{\\mathrm{T}}\\tag{3-1} $$ $$ p(y\\mid\\mathbf{z}) = \\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(y\\mid z_{k}, \\sigma_{k}^2)\\tag{3-2} $$ 上述公式中$ \\pi_{k} $是第$ k $个高斯分布的权重，满足$ \\sum_{k=1}^{K}\\pi_{k}=1 $，$ \\mathcal{N}(y\\mid z_{k}, \\sigma_{k}^2) $是第$ k $个高斯分布的概率密度。\n混合高斯分布的概率分布密度如图所示。\n上述条件概率经过$ \\log $化之后取负得到的损失函数为\n$$ \\begin{align} \\arg\\min -\\log p(y\\mid \\mathbf{z}) \u0026= \\arg\\min -\\log \\left[\\sum_{k=1}^K\\pi_{k}\\frac{1}{\\sqrt{ 2\\pi \\sigma_{k}^2 }}\\exp\\left[-\\frac{(y - z_{k})^2}{2\\sigma_{k}^2}\\right]\\right]\\quad \\mathrm{log\\text{-}sum\\text{-}exp\\:construct} \\\\ \u0026= \\arg\\min -\\log \\sum_{k=1}^K\\exp[a_{k}]\\quad \\left( a_{k} = \\log \\pi_{k} - \\frac{1}{2}\\log[2\\pi \\sigma_{k}^2] - \\frac{(y - z_{k})^2}{2\\sigma_{k^2}} \\right) \\\\ \u0026= \\arg\\min -m -\\log \\sum_{k=1}^K\\exp[a_{k} - m]\\quad \\left(m = \\max_k a_{k}\\right) \\end{align}\\tag{3-3} $$ $ \\mathrm{Laplace} $分布 $$ y\\sim \\mathrm{Laplace}(z, b)\\tag{4-1} $$ $$ p(y\\mid z) = \\frac{1}{2b}\\exp\\Big[ -\\frac{|y - z|}{b}\\Big]\\tag{4-2} $$ $ \\mathrm{Laplace} $分布概率密度分度如图所示，$ \\mu $是位置参数控制分布中心，$ b $是尺度参数控制分布的宽度。\n上述条件概率分布经过$ \\log $化之后取负对数得到对应的损失函数为\n$$ \\begin{align} \\arg\\min -\\log p(y\\mid z) \u0026= \\arg\\min\\log(2b) + \\frac{\\lvert y - z \\rvert}{b} \\end{align}\\tag{4-3} $$ $ \\mathrm{t}分布 $ $$ y\\sim \\mathrm{t}(\\nu, \\mathbf{z})\\quad \\mathbf{z}=[z_{1}, z_{2}]^{\\mathrm{T}}\\tag{5-1} $$ $$ p(y \\mid \\mathbf{z}) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi}\\, \\sigma \\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left[ 1 + \\frac{1}{\\nu} \\left(\\frac{y-z_{1}}{\\exp[z_{2}]}\\right)^2 \\right]^{-\\frac{\\nu+1}{2}}\\tag{5-2} $$ $ \\mathrm{t} $分布概率密度如图所示，其中$ \\mu $依旧表示位置参数，$ \\sigma\u003e0 $表示尺度参数，$ \\nu\u003e0 $表示自由度\n`$ \\mathrm{t} $`分布和正态分布（单变量）的概率密度函数在不同自由度对比如下 上述条件分布`$ \\log $`化之后取负值得到对应的损失函数为 $$ \\begin{align} \\arg\\min -\\log p(y\\mid \\mathbf{z}) \u0026= \\arg\\min z_{2} + \\frac{\\nu + 1}{2}\\log \\left(1 + \\frac{1}{\\nu}\\frac{\\left(y - z_{1}\\right)^2}{\\exp[2z_{2}]}\\right) + C \\quad \\left(C = -\\log\\Gamma\\left(\\tfrac{\\nu+1}{2}\\right)+\\log\\Gamma\\left(\\tfrac{\\nu}{2}\\right)+\\frac{1}{2}\\log(\\nu\\pi)\\right) \\\\ \u0026= \\arg\\min z_{2} + \\frac{\\nu + 1}{2}\\log \\left(1 + \\frac{1}{\\nu}\\frac{\\left(y - z_{1}\\right)^2}{\\exp[2z_{2}]}\\right) \\end{align}\\tag{5-3} $$ 指数分布 $$ y\\sim \\mathrm{Exponential}\\left( \\frac{1}{\\exp[z]} \\right) \\tag{6-1} $$ $$ p(y\\mid z) = \\begin{cases} \\frac{1}{\\exp[z]} \\exp\\left[ -\\frac{y}{\\exp[z]} \\right], \u0026 y \\geq 0 \\\\ 0, \u0026 y \u003c 0 \\end{cases}\\tag{6-2} $$ 指数分布的概率密度如图所示\n上述条件分布`$ \\log $`化之后取负值得到对应的损失函数为 $$ \\begin{align} \\arg\\min -\\log p(y\\mid z) \u0026= \\arg\\min z + y\\exp[-z] \\quad (y \\geq 0) \\end{align}\\tag{6-3} $$ $ \\mathrm{gamma}分布 $ $$ y\\sim \\mathrm{Gamma}(\\mathbf{z})\\quad \\mathbf{z}=[z_{1}, z_{2}]^{\\mathrm{T}}\\tag{7-1} $$ $$ p(y\\mid \\mathbf{z}) = \\begin{cases} \\frac{1}{\\Gamma(\\exp(z_{1}))\\exp[z_{2}]^{\\exp[z_{1}]}}y^{\\exp[z_{1}] - 1}\\exp\\left[ -\\frac{y}{\\exp[z_{2}]} \\right]\u0026y \\geq 0\\\\ 0, \u0026y \u003c 0 \\end{cases} $$ $ \\mathrm{gamma} $分布的概率密度如图所示，其中$ k\u003e0 $表示形状参数，$ \\theta\u003e0 $表示尺度参数。\n上述条件分布`$ \\log $`化之后取负值得到对应的损失函数为 $$ \\begin{align} \\arg\\min -\\log p(y\\mid \\mathbf{z}) \u0026= \\arg\\min \\log \\Gamma(\\exp[z_{1}]) + \\exp[z_{1}]z_{2} - (\\exp[z_{1}] - 1)\\log y + y\\exp[-z_{2}] \\quad (y \\geq 0) \\end{align}\\tag{7-3} $$ $ \\mathrm{beta}分布 $ $$ y\\sim \\mathrm{Beta}(\\mathbf{z})\\quad \\mathbf{z}=[z_{1}, z_{2}]^{\\mathrm{T}}\\tag{8-1} $$ $$ p(y\\mid \\mathbf{z}) = \\frac{1}{\\mathrm{B}(\\exp[z_{1}], \\exp[z_{2}])}y^{\\exp[z_{1}] - 1}(1-y)^{\\exp[z_{2}] - 1}\\tag{8-2} $$ $ \\mathrm{beta} $分布的概率密度函数如下所示，其中$ \\alpha\u003e0, \\beta\u003e0 $都表示形状参数\n上述条件分布$ \\log $化之后取负值得到对应的损失函数为\n$$ \\begin{aligned} \\arg\\min -\\log p(y\\mid \\mathbf{z}) \u0026= \\arg\\min\\underbrace{\\log \\Gamma(\\exp[z_{1}]) + \\log \\Gamma(\\exp[z_{2}]) - \\log \\Gamma(\\exp[z_{1}] + \\exp[z_{2}])}_{\\log \\mathrm{B(\\exp[z_{1}], \\exp[z_{2}])}} - (\\exp[z_{1}] - 1)\\log y - (\\exp[z_{2}] - 1)\\log(1 - y) \\\\ \u0026= \\arg\\min\\log \\mathrm{B}(\\exp[z_{1}], \\exp[z_{2}]) - (\\exp[z_{1}] - 1)\\log y - (\\exp[z_{2}] - 1)\\log(1 - y) \\quad (y \\geq 0) \\end{aligned}\\tag{8-3} $$ 离散随机变量分布 $ \\mathrm{Possion} $分布 $$ y\\sim \\mathrm{Possion}(\\exp[z])\\tag{9-1} $$ $$ p(y|z) = \\frac{\\exp[z]^{y}\\exp[-\\exp[z]]}{y!}\\tag{9-2} $$ 上述条件分布$ \\log $化之后取负值得到对应的损失函数为\n$$ \\begin{align} \\arg\\min -\\log p(y\\mid z) \u0026= \\arg\\min\\exp[z] - z\\exp[z] + C \\quad (C = \\log[y!])\\\\ \u0026= \\arg\\min\\exp[z] - z\\exp[z] \\end{align}\\tag{9-3} $$ $ \\mathrm{Bernoulli} $分布 $$ y\\sim \\mathrm{Bernoulli}(\\mathrm{sigmoid}(z))\\tag{10-1} $$ $$ p(y\\mid z) = \\mathrm{sigmoid}(z)^{y}(1 - \\mathrm{sigmoid}(z))^{1-y}\\tag{10-2} $$ 分类分布 $$ p(y\\mid\\mathbf{z})=\\mathrm{softmax}_{y}(\\mathbf{z})=\\frac{\\exp[z_{y}]}{\\sum_{y'=1}^{K}\\exp[z_{y'}]}\\tag{11-1} $$ 不同概率之间的距离 $ \\mathrm{Kullback-Leibler} $散度（$ \\mathrm{KL} $散度） $ \\mathrm{KL} $散度是一种衡量两个概率分布差异的指标。概率分布$ p(y) $和概率分布$ q(y) $之间的距离可以用$ \\mathrm{KL} $散度来进行评估\n$$ \\mathrm{D}_{\\mathrm{KL}}[p(y)||q(y))] = \\int_{-\\infty}^{\\infty}p(y)\\log[p(y)]dy - \\int_{-\\infty}^{\\infty}p(y)\\log[q(y)]dy $$ 两个多维正态分布之间的$ \\mathrm{KL} $散度，其中$ D $表示多维正态分布的维度数量\n$$ \\mathrm{D}_{\\mathrm{KL}}[\\mathcal{N}(\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1})||\\mathcal{N}(\\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma}_{2})] = \\frac{1}{2}\\left( \\log\\left[ \\frac{|\\boldsymbol{\\Sigma}_{2}|}{|\\boldsymbol{\\Sigma}_{1}|}\\right] - D + \\mathrm{tr}[\\boldsymbol{\\Sigma}_{2}^{-1}\\boldsymbol{\\Sigma}_{1}] + (\\boldsymbol{\\mu}_{2} - \\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol{\\Sigma}_{2}^{-1}(\\boldsymbol{\\mu}_{2} - \\boldsymbol{\\mu}_{1})\\right) $$ $ \\mathrm{Jensen-Shannon} $散度（$ \\mathrm{JS} $散度） $ \\mathrm{KL} $散度是反对称的($ \\mathrm{D}_{\\mathrm{KL}}[p(y)||q(y)]\\neq \\mathrm{D}_{\\mathrm{KL}}[q(y)||p(y)] $)，但是距离通常是对称的，因此通过$ \\mathrm{KL} $散度来构造一个对称化的距离度量$ \\mathrm{JS} $散度\n$$ \\mathrm{D}_{\\mathrm{JS}}[p(y)||q(y)] = \\frac{1}{2}\\mathrm{D}_{\\mathrm{KL}}\\left[ p(y)||\\frac{p(y) + q(y)}{2} \\right] + \\frac{1}{2}\\mathrm{D}_{\\mathrm{KL}}\\left[ q(y)||\\frac{p(y) + q(y)}{2} \\right] $$ 它的物理意义是$ p(y) $和$ q(y) $对两个分布平均值的平均散度。\n$ \\mathrm{Fréchet} $ 距离 两个概率分布$ p(y) $和$ q(y) $的$ \\mathrm{Fréchet} $ 距离的数学形式如下所示\n$$ \\mathrm{D}_{\\mathrm{Fr}}[p(x)||q(y)] = \\sqrt{ \\min_{\\pi(x,y)}\\left[\\int \\int \\pi(x, y)|x - y|^2dxdy \\right] } $$ 其中$ \\pi(x, y) $表示边缘分布$ p(x) $和$ q(y) $相容的联合分布集合。$ \\mathrm{Fréchet} $ 距离也可以被表述为累积分布曲线之间的最大距离的度量。\n两个多维正态分布之间的$ \\mathrm{Fréchet} $ 距离\n$$ \\mathrm{D}_{\\mathrm{Fr}/W_{2}}[\\mathcal{N}(\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1})||\\mathcal{N}(\\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma}_{2})] = |\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{2}|^2 + \\mathrm{tr}[\\boldsymbol{\\Sigma}_{1} + \\boldsymbol{\\Sigma}_{2} - 2(\\boldsymbol{\\Sigma}_{2}\\boldsymbol{\\Sigma}_{1})^{1/2}] $$ ","wordCount":"716","inLanguage":"en","image":"https://fordelkon.github.io/img/merge.png","datePublished":"2025-08-24T18:13:33+09:00","dateModified":"2025-08-24T18:13:33+09:00","author":{"@type":"Person","name":"DL Kong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fordelkon.github.io/teaching/probdis/"},"publisher":{"@type":"Organization","name":"DL Kong","logo":{"@type":"ImageObject","url":"https://fordelkon.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fordelkon.github.io/ accesskey=h title="DL Kong (Alt + H)"><img src=https://fordelkon.github.io/logo_filled_outlined_6.png alt="Site icon in header" aria-label=logo height=35>DL Kong</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button">
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://fordelkon.github.io/about/ title=About><span>About</span></a></li><li><a href=https://fordelkon.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://fordelkon.github.io/teaching/ title=Teaching><span>Teaching</span></a></li><li><a href=https://fordelkon.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fordelkon.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://fordelkon.github.io/teaching/>Teaching</a></div><h1 class=post-title>深度学习中常用的概率分布总结</h1><div class=post-meta><span title='2025-08-24 18:13:33 +0900 +0900'>August 8, 24033</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;DL Kong</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#常用概率分布>常用概率分布</a><ul><li><a href=#连续随机变量分布>连续随机变量分布</a><ul><li><a href=#正态分布单变量>正态分布（单变量）</a></li><li><a href=#正态分布多变量>正态分布（多变量）</a></li><li><a href=#混合高斯分布>混合高斯分布</a></li><li><a href=#-mathrmlaplace-分布><code>$ \mathrm{Laplace} $</code>分布</a></li><li><a href=#-mathrmt分布-><code>$ \mathrm{t}分布 $</code></a></li><li><a href=#指数分布>指数分布</a></li><li><a href=#-mathrmgamma分布-><code>$ \mathrm{gamma}分布 $</code></a></li><li><a href=#-mathrmbeta分布-><code>$ \mathrm{beta}分布 $</code></a></li></ul></li><li><a href=#离散随机变量分布>离散随机变量分布</a><ul><li><a href=#-mathrmpossion-分布><code>$ \mathrm{Possion} $</code>分布</a></li><li><a href=#-mathrmbernoulli-分布><code>$ \mathrm{Bernoulli} $</code>分布</a></li><li><a href=#分类分布>分类分布</a></li></ul></li></ul></li><li><a href=#不同概率之间的距离>不同概率之间的距离</a><ul><li><a href=#-mathrmkullback-leibler-散度-mathrmkl-散度><code>$ \mathrm{Kullback-Leibler} $</code>散度（<code>$ \mathrm{KL} $</code>散度）</a></li><li><a href=#-mathrmjensen-shannon-散度-mathrmjs-散度><code>$ \mathrm{Jensen-Shannon} $</code>散度（<code>$ \mathrm{JS} $</code>散度）</a></li><li><a href=#-mathrmfréchet--距离><code>$ \mathrm{Fréchet} $</code> 距离</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>在深度学习中我们通常使用具有良好数学性质的概率分布对已有数据分布进行建模，以便得到优美的损失函数形式方便模型进行反向传播，同时赋予模型实际的概率含义。在该章节中我们介绍深度学习中常用的概率分布以及不同概率分布之间的距离度量方法。假设我们已有数据集<code>$ \mathcal{D}=\lbrace(\mathbf{x}_{i}, y_{i})\rbrace_{i=1}^{N} $</code>或<code>$ \mathcal{D}=\lbrace\mathbf{x}_{i}\rbrace_{i=1}^{N} $</code>，主要区分于有标签和无标签的情况，<code>$ z $</code>或<code>$ \mathbf{z}$ </code>主要表示深度学习模型未经处理的输出。</p><h2 id=常用概率分布>常用概率分布<a hidden class=anchor aria-hidden=true href=#常用概率分布>#</a></h2><h3 id=连续随机变量分布>连续随机变量分布<a hidden class=anchor aria-hidden=true href=#连续随机变量分布>#</a></h3><h4 id=正态分布单变量>正态分布（单变量）<a hidden class=anchor aria-hidden=true href=#正态分布单变量>#</a></h4><div>$$
y\sim\mathcal{N}(z, \sigma^2)\tag{1-1}
$$</div><div>$$
p(y\mid z) = \frac{1}{\sqrt{ 2\pi \sigma^2 }}\exp\Big[ -\frac{(y-z)^2}{2\sigma^2}\Big]\tag{1-2}
$$</div><p>单变量正态分布概率密度函数曲线如图所示：</p><center><img src=./img/uni_norm.png alt=图片1 width=400></center><p>上述条件概率经过<code>$ \log $</code>化取负之后化简其对应的损失函数</p><div>$$
\begin{align}
\arg\min -\log p(y\mid z) &= \arg\min \Big[\frac{1}{2}\log [ 2\pi \sigma^2] + \frac{(y - z)^2}{2\sigma^2}\Big] \\
&= \arg\min [(y - z)^2] \quad (\sigma\: is \: constant)
\end{align}\tag{1-3}
$$</div><h4 id=正态分布多变量>正态分布（多变量）<a hidden class=anchor aria-hidden=true href=#正态分布多变量>#</a></h4><div>$$
\mathbf{y}\sim \mathcal{N}(\mathbf{z}, \Sigma)\tag{2-1}
$$</div><div>$$
p(\mathbf{y}\mid \mathbf{z}) = \frac{1}{(2\pi)^{K/2}|\Sigma|^{1/2}}\exp\Big[ -\frac{(\mathbf{y}-\mathbf{z})^{\mathrm{T}}\Sigma^{-1}(\mathbf{y}-\mathbf{z})}{2}\Big]\tag{2-2}
$$</div><p>二维正态分布概率密度等高线如图所示</p><center><img src=./img/bi_norm.png alt=图片1 width=400></center><p>同理，上述条件概率经过<code>$ \log $</code>化取负之后化简得到其对应的损失函数</p><div>$$
\begin{align}
\arg\min -\log p(\mathbf{y}\mid \mathbf{z}) &= \arg\min \left[\frac{K}{2}\log[2\pi] + \frac{1}{2}\log |\Sigma| + \frac{1}{2}(\mathbf{y} - \mathbf{z})^\mathrm{T}\Sigma^{-1}(\mathbf{y} - \mathbf{z})\right] \\
&= \arg\min \left[\frac{K}{2}\log[2\pi \sigma^2] + \frac{1}{2\sigma^2}\lvert \lvert \mathbf{y} - \mathbf{z} \rvert \rvert^2 \right]\quad (\Sigma = \sigma^2\mathbf{I}) \\
&= \arg\min \left[\lvert \lvert \mathbf{y} - \mathbf{z} \rvert \rvert^2 \right]\quad (\sigma\:is\: constant)
\end{align}\tag{2-3}
$$</div><h4 id=混合高斯分布>混合高斯分布<a hidden class=anchor aria-hidden=true href=#混合高斯分布>#</a></h4><div>$$
y\sim \mathrm{GMM}(\mathbf{z},\boldsymbol{\sigma}^2)\quad\mathbf{z}=[z_{1}, \dots, z_{K}]^{\mathrm{T}}\tag{3-1}
$$</div><div>$$
p(y\mid\mathbf{z}) = \sum_{k=1}^{K}\pi_{k}\mathcal{N}(y\mid z_{k}, \sigma_{k}^2)\tag{3-2}
$$</div><p>上述公式中<code>$ \pi_{k} $</code>是第<code>$ k $</code>个高斯分布的权重，满足<code>$ \sum_{k=1}^{K}\pi_{k}=1 $</code>，<code>$ \mathcal{N}(y\mid z_{k}, \sigma_{k}^2) $</code>是第<code>$ k $</code>个高斯分布的概率密度。</p><p>混合高斯分布的概率分布密度如图所示。</p><center><img src=./img/gmm.png alt=图片1 width=400></center><p>上述条件概率经过<code>$ \log $</code>化之后取负得到的损失函数为</p><div>$$
\begin{align}
\arg\min -\log p(y\mid \mathbf{z}) &= \arg\min -\log \left[\sum_{k=1}^K\pi_{k}\frac{1}{\sqrt{ 2\pi \sigma_{k}^2 }}\exp\left[-\frac{(y - z_{k})^2}{2\sigma_{k}^2}\right]\right]\quad \mathrm{log\text{-}sum\text{-}exp\:construct} \\
&= \arg\min -\log \sum_{k=1}^K\exp[a_{k}]\quad \left( a_{k} = \log \pi_{k} - \frac{1}{2}\log[2\pi \sigma_{k}^2] - \frac{(y - z_{k})^2}{2\sigma_{k^2}} \right) \\
&= \arg\min -m -\log \sum_{k=1}^K\exp[a_{k} - m]\quad \left(m = \max_k a_{k}\right)
\end{align}\tag{3-3}
$$</div><h4 id=-mathrmlaplace-分布><code>$ \mathrm{Laplace} $</code>分布<a hidden class=anchor aria-hidden=true href=#-mathrmlaplace-分布>#</a></h4><div>$$
y\sim \mathrm{Laplace}(z, b)\tag{4-1}
$$</div><div>$$
p(y\mid z) = \frac{1}{2b}\exp\Big[ -\frac{|y - z|}{b}\Big]\tag{4-2}
$$</div><p><code>$ \mathrm{Laplace} $</code>分布概率密度分度如图所示，<code>$ \mu $</code>是位置参数控制分布中心，<code>$ b $</code>是尺度参数控制分布的宽度。</p><center><img src=./img/laplace.png alt=图片1 width=400></center><p>上述条件概率分布经过<code>$ \log $</code>化之后取负对数得到对应的损失函数为</p><div>$$
\begin{align}
\arg\min -\log p(y\mid z) &= \arg\min\log(2b) + \frac{\lvert y - z \rvert}{b}
\end{align}\tag{4-3}
$$</div><h4 id=-mathrmt分布-><code>$ \mathrm{t}分布 $</code><a hidden class=anchor aria-hidden=true href=#-mathrmt分布->#</a></h4><div>$$
y\sim \mathrm{t}(\nu, \mathbf{z})\quad \mathbf{z}=[z_{1}, z_{2}]^{\mathrm{T}}\tag{5-1}
$$</div><div>$$
p(y \mid \mathbf{z}) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu \pi}\, \sigma \, \Gamma\left(\frac{\nu}{2}\right)} \left[ 1 + \frac{1}{\nu} \left(\frac{y-z_{1}}{\exp[z_{2}]}\right)^2 \right]^{-\frac{\nu+1}{2}}\tag{5-2}
$$</div><p><code>$ \mathrm{t} $</code>分布概率密度如图所示，其中<code>$ \mu $</code>依旧表示位置参数，<code>$ \sigma>0 $</code>表示尺度参数，<code>$ \nu>0 $</code>表示自由度</p><center><img src=./img/t.png alt=图片1 width=400></center>`$ \mathrm{t} $`分布和正态分布（单变量）的概率密度函数在不同自由度对比如下<center><img src=./img/t_vs_norm.png alt=图片1 width=400></center>上述条件分布`$ \log $`化之后取负值得到对应的损失函数为<div>$$
\begin{align}
\arg\min -\log p(y\mid \mathbf{z}) &= \arg\min z_{2} + \frac{\nu + 1}{2}\log \left(1 + \frac{1}{\nu}\frac{\left(y - z_{1}\right)^2}{\exp[2z_{2}]}\right) + C \quad \left(C = -\log\Gamma\left(\tfrac{\nu+1}{2}\right)+\log\Gamma\left(\tfrac{\nu}{2}\right)+\frac{1}{2}\log(\nu\pi)\right) \\
&= \arg\min z_{2} + \frac{\nu + 1}{2}\log \left(1 + \frac{1}{\nu}\frac{\left(y - z_{1}\right)^2}{\exp[2z_{2}]}\right)
\end{align}\tag{5-3}
$$</div><h4 id=指数分布>指数分布<a hidden class=anchor aria-hidden=true href=#指数分布>#</a></h4><div>$$
y\sim \mathrm{Exponential}\left( \frac{1}{\exp[z]} \right) \tag{6-1}
$$</div><div>$$
p(y\mid z) = \begin{cases} \frac{1}{\exp[z]} \exp\left[ -\frac{y}{\exp[z]} \right], & y \geq 0 \\ 0, & y < 0 \end{cases}\tag{6-2}
$$</div><p>指数分布的概率密度如图所示</p><center><img src=./img/expon.png alt=图片1 width=400></center>上述条件分布`$ \log $`化之后取负值得到对应的损失函数为<div>$$
\begin{align}
\arg\min -\log p(y\mid z) &= \arg\min z + y\exp[-z] \quad (y \geq 0)
\end{align}\tag{6-3}
$$</div><h4 id=-mathrmgamma分布-><code>$ \mathrm{gamma}分布 $</code><a hidden class=anchor aria-hidden=true href=#-mathrmgamma分布->#</a></h4><div>$$
y\sim \mathrm{Gamma}(\mathbf{z})\quad \mathbf{z}=[z_{1}, z_{2}]^{\mathrm{T}}\tag{7-1}
$$</div><div>$$
p(y\mid \mathbf{z}) = \begin{cases}
\frac{1}{\Gamma(\exp(z_{1}))\exp[z_{2}]^{\exp[z_{1}]}}y^{\exp[z_{1}] - 1}\exp\left[ -\frac{y}{\exp[z_{2}]} \right]&y \geq 0\\
0, &y < 0
\end{cases}
$$</div><p><code>$ \mathrm{gamma} $</code>分布的概率密度如图所示，其中<code>$ k>0 $</code>表示形状参数，<code>$ \theta>0 $</code>表示尺度参数。</p><center><img src=./img/gamma.png alt=图片1 width=400></center>上述条件分布`$ \log $`化之后取负值得到对应的损失函数为<div>$$
\begin{align}
\arg\min -\log p(y\mid \mathbf{z}) &= \arg\min \log \Gamma(\exp[z_{1}]) + \exp[z_{1}]z_{2} - (\exp[z_{1}] - 1)\log y + y\exp[-z_{2}] \quad (y \geq 0)
\end{align}\tag{7-3}
$$</div><h4 id=-mathrmbeta分布-><code>$ \mathrm{beta}分布 $</code><a hidden class=anchor aria-hidden=true href=#-mathrmbeta分布->#</a></h4><div>$$
y\sim \mathrm{Beta}(\mathbf{z})\quad \mathbf{z}=[z_{1}, z_{2}]^{\mathrm{T}}\tag{8-1}
$$</div><div>$$
p(y\mid \mathbf{z}) = \frac{1}{\mathrm{B}(\exp[z_{1}], \exp[z_{2}])}y^{\exp[z_{1}] - 1}(1-y)^{\exp[z_{2}] - 1}\tag{8-2}
$$</div><p><code>$ \mathrm{beta} $</code>分布的概率密度函数如下所示，其中<code>$ \alpha>0, \beta>0 $</code>都表示形状参数</p><center><img src=./img/beta.png alt=图片1 width=400></center><p>上述条件分布<code>$ \log $</code>化之后取负值得到对应的损失函数为</p><div>$$
\begin{aligned}
\arg\min -\log p(y\mid \mathbf{z}) &= \arg\min\underbrace{\log \Gamma(\exp[z_{1}]) + \log \Gamma(\exp[z_{2}]) - \log \Gamma(\exp[z_{1}] + \exp[z_{2}])}_{\log \mathrm{B(\exp[z_{1}], \exp[z_{2}])}} - (\exp[z_{1}] - 1)\log y - (\exp[z_{2}] - 1)\log(1 - y) \\
&= \arg\min\log \mathrm{B}(\exp[z_{1}], \exp[z_{2}]) - (\exp[z_{1}] - 1)\log y - (\exp[z_{2}] - 1)\log(1 - y) \quad (y \geq 0)
\end{aligned}\tag{8-3}
$$</div><h3 id=离散随机变量分布>离散随机变量分布<a hidden class=anchor aria-hidden=true href=#离散随机变量分布>#</a></h3><h4 id=-mathrmpossion-分布><code>$ \mathrm{Possion} $</code>分布<a hidden class=anchor aria-hidden=true href=#-mathrmpossion-分布>#</a></h4><div>$$
y\sim \mathrm{Possion}(\exp[z])\tag{9-1}
$$</div><div>$$
p(y|z) = \frac{\exp[z]^{y}\exp[-\exp[z]]}{y!}\tag{9-2}
$$</div><p>上述条件分布<code>$ \log $</code>化之后取负值得到对应的损失函数为</p><div>$$
\begin{align}
\arg\min -\log p(y\mid z) &= \arg\min\exp[z] - z\exp[z] + C \quad (C = \log[y!])\\
&= \arg\min\exp[z] - z\exp[z]
\end{align}\tag{9-3}
$$</div><h4 id=-mathrmbernoulli-分布><code>$ \mathrm{Bernoulli} $</code>分布<a hidden class=anchor aria-hidden=true href=#-mathrmbernoulli-分布>#</a></h4><div>$$
y\sim \mathrm{Bernoulli}(\mathrm{sigmoid}(z))\tag{10-1}
$$</div><div>$$
p(y\mid z) = \mathrm{sigmoid}(z)^{y}(1 - \mathrm{sigmoid}(z))^{1-y}\tag{10-2}
$$</div><h4 id=分类分布>分类分布<a hidden class=anchor aria-hidden=true href=#分类分布>#</a></h4><div>$$
p(y\mid\mathbf{z})=\mathrm{softmax}_{y}(\mathbf{z})=\frac{\exp[z_{y}]}{\sum_{y'=1}^{K}\exp[z_{y'}]}\tag{11-1}
$$</div><h2 id=不同概率之间的距离>不同概率之间的距离<a hidden class=anchor aria-hidden=true href=#不同概率之间的距离>#</a></h2><h3 id=-mathrmkullback-leibler-散度-mathrmkl-散度><code>$ \mathrm{Kullback-Leibler} $</code>散度（<code>$ \mathrm{KL} $</code>散度）<a hidden class=anchor aria-hidden=true href=#-mathrmkullback-leibler-散度-mathrmkl-散度>#</a></h3><p><code>$ \mathrm{KL} $</code>散度是一种衡量两个概率分布差异的指标。概率分布<code>$ p(y) $</code>和概率分布<code>$ q(y) $</code>之间的距离可以用<code>$ \mathrm{KL} $</code>散度来进行评估</p><div>$$
\mathrm{D}_{\mathrm{KL}}[p(y)||q(y))] = \int_{-\infty}^{\infty}p(y)\log[p(y)]dy - \int_{-\infty}^{\infty}p(y)\log[q(y)]dy
$$</div><p>两个多维正态分布之间的<code>$ \mathrm{KL} $</code>散度，其中<code>$ D $</code>表示多维正态分布的维度数量</p><div>$$
\mathrm{D}_{\mathrm{KL}}[\mathcal{N}(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1})||\mathcal{N}(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2})] = \frac{1}{2}\left( \log\left[ \frac{|\boldsymbol{\Sigma}_{2}|}{|\boldsymbol{\Sigma}_{1}|}\right] - D + \mathrm{tr}[\boldsymbol{\Sigma}_{2}^{-1}\boldsymbol{\Sigma}_{1}] + (\boldsymbol{\mu}_{2} - \boldsymbol{\mu}_{1})^{\mathrm{T}}\boldsymbol{\Sigma}_{2}^{-1}(\boldsymbol{\mu}_{2} - \boldsymbol{\mu}_{1})\right)
$$</div><h3 id=-mathrmjensen-shannon-散度-mathrmjs-散度><code>$ \mathrm{Jensen-Shannon} $</code>散度（<code>$ \mathrm{JS} $</code>散度）<a hidden class=anchor aria-hidden=true href=#-mathrmjensen-shannon-散度-mathrmjs-散度>#</a></h3><p><code>$ \mathrm{KL} $</code>散度是反对称的(<code>$ \mathrm{D}_{\mathrm{KL}}[p(y)||q(y)]\neq \mathrm{D}_{\mathrm{KL}}[q(y)||p(y)] $</code>)，但是距离通常是对称的，因此通过<code>$ \mathrm{KL} $</code>散度来构造一个对称化的距离度量<code>$ \mathrm{JS} $</code>散度</p><div>$$
\mathrm{D}_{\mathrm{JS}}[p(y)||q(y)] = \frac{1}{2}\mathrm{D}_{\mathrm{KL}}\left[ p(y)||\frac{p(y) + q(y)}{2} \right] + \frac{1}{2}\mathrm{D}_{\mathrm{KL}}\left[ q(y)||\frac{p(y) + q(y)}{2} \right]
$$</div><p>它的物理意义是<code>$ p(y) $</code>和<code>$ q(y) $</code>对两个分布平均值的平均散度。</p><h3 id=-mathrmfréchet--距离><code>$ \mathrm{Fréchet} $</code> 距离<a hidden class=anchor aria-hidden=true href=#-mathrmfréchet--距离>#</a></h3><p>两个概率分布<code>$ p(y) $</code>和<code>$ q(y) $</code>的<code>$ \mathrm{Fréchet} $</code> 距离的数学形式如下所示</p><div>$$
\mathrm{D}_{\mathrm{Fr}}[p(x)||q(y)] = \sqrt{ \min_{\pi(x,y)}\left[\int \int \pi(x, y)|x - y|^2dxdy \right] }
$$</div><p>其中<code>$ \pi(x, y) $</code>表示边缘分布<code>$ p(x) $</code>和<code>$ q(y) $</code>相容的联合分布集合。<code>$ \mathrm{Fréchet} $</code> 距离也可以被表述为累积分布曲线之间的最大距离的度量。</p><p>两个多维正态分布之间的<code>$ \mathrm{Fréchet} $</code> 距离</p><div>$$
\mathrm{D}_{\mathrm{Fr}/W_{2}}[\mathcal{N}(\boldsymbol{\mu}_{1}, \boldsymbol{\Sigma}_{1})||\mathcal{N}(\boldsymbol{\mu}_{2}, \boldsymbol{\Sigma}_{2})] = |\boldsymbol{\mu}_{1} - \boldsymbol{\mu}_{2}|^2 + \mathrm{tr}[\boldsymbol{\Sigma}_{1} + \boldsymbol{\Sigma}_{2} - 2(\boldsymbol{\Sigma}_{2}\boldsymbol{\Sigma}_{1})^{1/2}]
$$</div></div><footer class=post-footer><ul class=post-tags><li><a href=https://fordelkon.github.io/tags/probability/>Probability</a></li></ul><nav class=paginav><a class=prev href=https://fordelkon.github.io/posts/git_intro/><span class=title>« Prev</span><br><span>Git详解</span>
</a><a class=next href=https://fordelkon.github.io/posts/cmake_intro_2/><span class=title>Next »</span><br><span>使用CMake的艺术2</span></a></nav></footer><div class=giscus_comments><script src=https://giscus.app/client.js data-repo=jesse-wei/jessewei.dev-PaperMod data-repo-id=R_kgDOJhJgPg data-category=Comments data-category-id=DIC_kwDOJhJgPs4CWei3 data-mapping=pathname data-strict=1 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></div><script async>document.querySelector("div.giscus_comments > script").setAttribute("data-theme",localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme"):window.matchMedia("(prefers-color-scheme: dark)").matches?"transparent_dark":"light"),document.querySelector("#theme-toggle").addEventListener("click",()=>{let e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:{setConfig:{theme:localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme")==="dark"?"light":"transparent_dark":document.body.className.includes("dark")?"light":"transparent_dark"}}},"https://giscus.app")})</script></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/fordelkon rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://x.com/fordelkon rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=mailto:dlkong201893@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://fordelkon.github.io/>DL Kong</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>