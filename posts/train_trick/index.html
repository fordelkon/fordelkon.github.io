<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深度学习训练技巧 | DL Kong</title><meta name=keywords content="DL,Python"><meta name=description content="进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。"><meta name=author content="DL Kong"><link rel=canonical href=https://fordelkon.github.io/posts/train_trick/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.c7a0847263ef81d155d535eb3508757a49b1a3680807f60ed1f64e9d4a686070.css integrity="sha256-x6CEcmPvgdFV1TXrNQh1ekmxo2gIB/YO0fZOnUpoYHA=" rel="preload stylesheet" as=style><link rel=icon href=https://fordelkon.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://fordelkon.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://fordelkon.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://fordelkon.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fordelkon.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://fordelkon.github.io/posts/train_trick/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><link rel=stylesheet href=/css/extended/xcode.css media="(prefers-color-scheme: light)"><link rel=stylesheet href=/css/extended/monokai.css media="(prefers-color-scheme: dark)"><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT")}</script><meta property="og:url" content="https://fordelkon.github.io/posts/train_trick/"><meta property="og:site_name" content="DL Kong"><meta property="og:title" content="深度学习训练技巧"><meta property="og:description" content="进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-01T10:36:33+09:00"><meta property="article:modified_time" content="2025-09-01T10:36:33+09:00"><meta property="article:tag" content="DL"><meta property="article:tag" content="Python"><meta property="og:image" content="https://fordelkon.github.io/img/optimizers.gif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fordelkon.github.io/img/optimizers.gif"><meta name=twitter:title content="深度学习训练技巧"><meta name=twitter:description content="进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fordelkon.github.io/posts/"},{"@type":"ListItem","position":2,"name":"深度学习训练技巧","item":"https://fordelkon.github.io/posts/train_trick/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深度学习训练技巧","name":"深度学习训练技巧","description":"进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。\n","keywords":["DL","Python"],"articleBody":"进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。\n优化器 优化器是模型训练过程中一个很重要的成分，使用不同种类的优化器有时会对训练的过程产生很大的影响，同时优化器中一些超参数的选择有时候也会对训练过程产生很大的影响。下面介绍一些常用的优化器以及其中的超参数介绍：\n介绍优化器之前，我们要先定义训练集$ \\mathcal{D}_{train} $\n$$ \\mathcal{D}_{train} = \\begin{cases} \\lbrace \\mathbf{x_{i}}\\rbrace_{i=1}^{N_{train}} \u0026 no\\:labels \\\\ \\lbrace (\\mathbf{x}_{i}, y_{i})\\rbrace_{i=1}^{N_{train}} \u0026 labels \\end{cases} $$ 然后从训练集中采集包含$ B_{s} $个样本的小批量$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $，下面在上面的设定下介绍各个优化器，为了适应有标签和无标签的情况最终所计算的损失我们统一记为$ \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\theta}) $。\nSGD优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{\\theta}_{t + 1} = \\boldsymbol{\\theta}_{t} - \\eta\\: \\boldsymbol{g}_{t}\\quad \u0026\\text{(update the parameters)} \\end{array} $$ 上述公式中$ \\eta $表示学习率，$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。\n使用Nesterov动量的SGD优化器 $$ \\begin{array}{l} \\textcolor{red}{\\tilde{\\boldsymbol{\\theta}}_{t}} = \\boldsymbol{\\theta}_{t} + \\alpha\\textcolor{blue}{\\boldsymbol{v}_{t}} \\quad \u0026\\text{(look ahead to the future position)} \\\\ \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\textcolor{red}{\\tilde{\\boldsymbol{\\theta}}_{t}})\\quad \u0026\\text{(compute gradient at the lookahead position)} \\\\ \\textcolor{blue}{\\boldsymbol{v}_{t+1}} = \\alpha\\textcolor{blue}{\\boldsymbol{v}_{t}} - \\eta\\boldsymbol{g}_{t} \\quad \u0026\\text{(update the velocity)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\textcolor{blue}{\\boldsymbol{v}_{t+1}}\\quad \u0026\\text{(update the parameters)} \\end{array} $$ 上述公式中$ \\eta $表示学习率，$ \\alpha $表示动量参数，$ \\boldsymbol{v}_{0}=\\mathbf{0} $表示初始速度。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。\nRMSProp优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{r}_{t+1} = \\rho\\boldsymbol{r}_{t} + (1 - \\rho)\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t} \\quad \u0026\\text{(update the running average of squared gradients)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\frac{\\eta}{\\sqrt{ \\delta + \\boldsymbol{r}_{t+1} }}\\circ \\boldsymbol{g}_{t} \\quad \u0026\\text{(update the parameters with the adaptive scaling)} \\end{array} $$ 上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho $表示衰减速率，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。\n使用Nesterov动量的RMSProp优化器 $$ \\begin{array}{l} \\textcolor{red}{\\tilde{\\boldsymbol{\\theta}}_{t}} = \\boldsymbol{\\theta}_{t} + \\alpha\\textcolor{blue}{\\boldsymbol{v}_{t}} \\quad \u0026\\text{(look ahead to the future position)} \\\\ \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\textcolor{red}{\\tilde{\\boldsymbol{\\theta}}_{t}})\\quad \u0026\\text{(compute gradient at the lookahead position)} \\\\ \\textcolor{purple}{\\boldsymbol{r}_{t+1}} = \\rho\\textcolor{purple}{\\boldsymbol{r}_{t}} + (1 - \\rho)\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t} \\quad \u0026\\text{(update the running average of squared gradients)} \\\\ \\textcolor{blue}{\\boldsymbol{v}_{t+1}} = \\alpha\\textcolor{blue}{\\boldsymbol{v}_{t}} - \\frac{\\eta}{\\sqrt{ \\delta + \\textcolor{purple}{\\boldsymbol{r}_{t+1}} }}\\circ\\boldsymbol{g}_{t} \\quad \u0026\\text{(update the velocity with adaptive scaling)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\textcolor{blue}{\\boldsymbol{v}_{t+1}}\\quad \u0026\\text{(update the parameters)} \\end{array} $$ 上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho $表示衰减速率，$ \\alpha $表示动量参数，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量，$ \\boldsymbol{v}_{0}=\\mathbf{0} $表示初始速度。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。\nAdam优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{s}_{t+1} = \\rho_{1}\\boldsymbol{s}_{t} + (1 - \\rho_{1})\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased first momentum)} \\\\ \\boldsymbol{r}_{t+1} = \\rho_{2}\\boldsymbol{r}_{t} + (1 - \\rho_{2})\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased second momentum)} \\\\ \\hat{\\boldsymbol{s}}_{t+1}, \\hat{\\boldsymbol{r}}_{t+1} = \\frac{\\boldsymbol{s}_{t+1}}{1 - \\rho_{1}^{t + 1}}, \\frac{\\boldsymbol{r}_{t+1}}{1 - \\rho_{2}^{t + 1}}\\quad\u0026\\text{(bias correction of first momentum and second momentum)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta \\:\\cdot\\frac{\\hat{\\boldsymbol{s}}_{t+1}}{\\delta + \\sqrt{ \\hat{\\boldsymbol{r}}_{t+1} }} \\quad\u0026\\text{(update the parameters)} \\end{array} $$ 上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho_{1} $和$ \\rho_{2} $表示一阶矩估计和二阶矩估计的衰减速率，$ \\boldsymbol{s}_{0}=\\mathbf{0} $表示初始累积梯度变量，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。\n使用Nesterov动量的Adam优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{s}_{t+1} = \\rho_{1}\\boldsymbol{s}_{t} + (1 - \\rho_{1})\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased first momentum)} \\\\ \\boldsymbol{r}_{t+1} = \\rho_{2}\\boldsymbol{r}_{t} + (1 - \\rho_{2})\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased second momentum)} \\\\ \\hat{\\boldsymbol{s}}_{t+1}, \\hat{\\boldsymbol{r}}_{t+1} = \\frac{\\boldsymbol{s}_{t+1}}{1 - \\rho_{1}^{t + 1}}, \\frac{\\boldsymbol{r}_{t+1}}{1 - \\rho_{2}^{t + 1}}\\quad\u0026\\text{(bias correction of first momentum and second momentum)} \\\\ \\textcolor{red}{\\tilde{\\boldsymbol{s}}_{t+1}}=\\rho_{1}\\hat{\\boldsymbol{s}}_{t+1}+\\frac{1-\\rho_{1}}{1-\\rho_{1}^{t+1}}\\boldsymbol{g}_{t}\\quad\u0026\\text{(lookahead adjustment of first momentum)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta \\:\\cdot\\frac{\\textcolor{red}{\\tilde{\\boldsymbol{s}}_{t+1}}}{\\delta + \\sqrt{ \\hat{\\boldsymbol{r}}_{t+1} }} \\quad\u0026\\text{(update the parameters)} \\end{array} $$ 同样上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho_{1} $和$ \\rho_{2} $表示一阶矩估计和二阶矩估计的衰减速率，$ \\boldsymbol{s}_{0}=\\mathbf{0} $表示初始累积梯度变量，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。使用Nesterov动量的Adam优化器与Adam的区别：提前预知一阶矩动量$ \\tilde{\\boldsymbol{s}}_{t+1} $。\nAdamax优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{s}_{t+1} = \\rho_{1}\\boldsymbol{s}_{t} + (1 - \\rho_{1})\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased first momentum)} \\\\ \\boldsymbol{r}_{t+1} = \\max(\\rho_{2}\\boldsymbol{r}_{t}, \\lvert \\boldsymbol{g}_{t} \\rvert )\\quad\u0026\\text{(update exponentially weighted infinity norm)} \\\\ \\hat{\\boldsymbol{s}}_{t+1} = \\frac{\\boldsymbol{s}_{t+1}}{1 - \\rho_{1}^{t + 1}}\\quad\u0026\\text{(bias correction of the first momentum)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta \\:\\cdot\\frac{\\hat{\\boldsymbol{s}}_{t+1}}{\\delta + \\boldsymbol{r}_{t+1}} \\quad\u0026\\text{(update the parameters)} \\end{array} $$ 同样上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho_{1} $和$ \\rho_{2} $表示一阶矩估计和无穷范数的衰减速率，$ \\boldsymbol{s}_{0}=\\mathbf{0} $表示初始累积梯度变量，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度无穷范数变量。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。Adamax优化器和Adam优化器的区别：使用无穷范数来代替二阶矩估计。\nAdamW优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{s}_{t+1} = \\rho_{1}\\boldsymbol{s}_{t} + (1 - \\rho_{1})\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased first momentum)} \\\\ \\boldsymbol{r}_{t+1} = \\rho_{2}\\boldsymbol{r}_{t} + (1 - \\rho_{2})\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased second momentum)} \\\\ \\hat{\\boldsymbol{s}}_{t+1}, \\hat{\\boldsymbol{r}}_{t+1} = \\frac{\\boldsymbol{s}_{t+1}}{1 - \\rho_{1}^{t + 1}}, \\frac{\\boldsymbol{r}_{t+1}}{1 - \\rho_{2}^{t + 1}}\\quad\u0026\\text{(bias correction of first momentum and second momentum)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta \\:\\cdot\\frac{\\hat{\\boldsymbol{s}}_{t+1}}{\\delta + \\sqrt{ \\hat{\\boldsymbol{r}}_{t+1} }}-\\eta\\:\\cdot \\lambda\\boldsymbol{\\theta}_{t} \\quad\u0026\\text{(separates weight decay from adaptive gradient scaling, ensuring true L2 regularization)} \\end{array} $$ 同样上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho_{1} $和$ \\rho_{2} $表示一阶矩估计和二阶矩估计的衰减速率，$ \\lambda $表示$ \\text{L2} $正则项系数。$ \\boldsymbol{s}_{0}=\\mathbf{0} $表示初始累积梯度变量，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。AdamW优化器和Adam优化器的主要区别：把$ \\text{L2} $正则化从梯度更新中分离出来，防止正则化不纯粹。\nRAdam优化器 $$ \\begin{array}{l} \\boldsymbol{g}_{t} = \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t})\\quad \u0026\\text{(compute gradient at the current position)} \\\\ \\boldsymbol{s}_{t+1} = \\rho_{1}\\boldsymbol{s}_{t} + (1 - \\rho_{1})\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased first momentum)} \\\\ \\boldsymbol{r}_{t+1} = \\rho_{2}\\boldsymbol{r}_{t} + (1 - \\rho_{2})\\boldsymbol{g}_{t}\\circ\\boldsymbol{g}_{t}\\quad\u0026\\text{(update the biased second momentum)} \\\\ \\hat{\\boldsymbol{s}}_{t+1}, \\hat{\\boldsymbol{r}}_{t+1} = \\frac{\\boldsymbol{s}_{t+1}}{1 - \\rho_{1}^{t + 1}}, \\frac{\\boldsymbol{r}_{t+1}}{1 - \\rho_{2}^{t + 1}}\\quad\u0026\\text{(bias correction of first momentum and the second momentum)} \\\\ \\gamma_{t + 1} = \\gamma_{\\infty} - \\frac{2(t+1)\\rho_{2}^{t+1}}{1 - \\rho_{2}^{t+1}}\\quad \u0026\\text{(compute the length of the approximated SMA)} \\\\ \\begin{cases} \\psi_{t+1} = \\sqrt{ \\frac{(\\gamma_{t+1} - 4)(\\gamma_{t+1} - 2)\\gamma_{\\infty}}{(\\gamma_{\\infty} - 4)(\\gamma_{\\infty} - 2)\\gamma_{t+1}} }\\quad\u0026\\text{(compute the variance rectification term)} \\\\ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta\\:\\cdot \\psi_{t+1}\\frac{\\hat{\\boldsymbol{s}}_{t+1}}{\\delta + \\sqrt{ \\hat{\\boldsymbol{r}}_{t+1}} } \\quad\u0026\\text{(update the parameters with adaptive momentum)} \\end{cases}\\quad\u0026(\\text{if}\\:\\gamma_{t+1} \u003e 4) \\\\ \\begin{cases} \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\eta\\:\\cdot \\hat{\\boldsymbol{s}}_{t+1}\\quad\\quad\\quad\\quad\\quad\u0026\\text{(update the parameters with un-adaptive momentum)} \\end{cases} \\quad\u0026\\text{(else)} \\end{array} $$ 同样上述公式中$ \\eta $表示学习率，$ \\delta $表示一个较小的常数，用于被小数除时的数值稳定，$ \\rho_{1} $和$ \\rho_{2} $表示一阶矩估计和二阶矩估计的衰减速率。$ \\boldsymbol{s}_{0}=\\mathbf{0} $表示初始累积梯度变量，$ \\boldsymbol{r}_{0}=\\mathbf{0} $表示初始累积梯度平方变量，$ \\gamma_{\\infty}=\\frac{2}{1-\\rho_{2}} - 1 $表示计算的近似SMA的最大长度。$ \\boldsymbol{\\theta}_{t} $表示第$ t $步时的模型参数，$ \\boldsymbol{\\theta}_{t+1} $表示在处理过小批量数据$ \\lbrace\\mathbf{x}^{1}, \\mathbf{x}^2, \\dots, \\mathbf{x}^{B_{s}}\\rbrace $之后第$ t+1 $步的模型参数。其与Adam优化器的区别是)，RAdam提出了一个方差校正因子$ \\psi $(rectification term)，自动调整有效学习率，让优化在初期更加稳定。\n调度器 上面一节中优化器中的超参数学习率$ \\eta $我们设定成了固定值，但是在真正实践中我们可以设定学习率$ \\eta $随着$ \\text{epoch} $变化($ \\text{epoch-based} $)或者随着$ \\text{step} $变化($ \\text{step-based} $)，这些变化我们可以通过调度器来进行实现，下面我们介绍一些常见的调度器。\n余弦调度器 由图可知，`$ \\text{step-based} $`的余弦调度器在每个`$ \\text{step} $`都会发生变化，而`$ \\text{epoch-based} $`的余弦调度器在每个`$ \\text{epoch} $`发生变化，而在相邻之间的`$ \\text{step} $`区间不会发生变化，通常`$ \\text{1 epoch}=\\text{1 step}\\times \\text{num batches} $`。 $$ \\eta_{t} = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left( 1 + \\cos\\left( \\frac{t}{T_{\\text{max}}} \\pi\\right) \\right) $$ 其中$ t $表示调度的粒度，既可以是$ \\text{step} $级别又可以是$ \\text{epoch} $级别。\n线性调度器 同余弦调度器的绘制说明，线性调度器的公式为 $$ \\eta_{t} = \\eta_{\\text{max}} - \\frac{t}{T_{\\text{max}}}(\\eta_{\\text{max}} - \\eta_{\\text{min}}) $$ 其中$ t $表示调度的粒度，既可以是$ \\text{step} $级别又可以是$ \\text{epoch} $级别。\n多项式调度器 同余弦调度器绘制说明，多项式调度器的公式为 $$ \\eta_{t} = \\eta_{\\text{min}} + (\\eta_{\\text{max}} - \\eta_{\\text{min}})\\left( 1 - \\frac{t}{T_{\\text{max}}} \\right)^p $$ 其中$ t $表示调度的粒度，既可以是$ \\text{step} $级别又可以是$ \\text{epoch} $级别。$ p $表示多项式幂指数。\n预热调度器 预热调度器一般是使用`$ \\text{step} $`粒度级别的线性增长调度。也就是上图中的5个`$ \\text{epoch} $`之前的过程都属于`$ \\text{warmup} $`过程。 梯度累积优化 当使用单卡GPU进行模型训练时当我们的批次大小设定过大时会出现显存不足的情况，所以单卡GPU往往设定的批次大小偏小以避免显存爆炸，但是小批量的训练方式往往会导致多个批次计算的梯度不稳定，最终导致损失的收敛不稳定。为了解决上述困境，一个有效的解决方法就是使用梯度累积优化。\n假设小批量设定为$ b_{s} $，累积的大批量设定为$ B_{s} $，习惯上$ B_{s}=n\\:\\cdot b_{s} $，$ n $表示梯度累积的$ \\text{step} $数。我们可以证明$ n $个$ b_{s} $批次梯度累积优化等价于一次$ B_{s} $批次梯度优化。\n$$ \\begin{align} \\boldsymbol{g}_{t} \u0026= \\frac{1}{n}\\sum_{i=1}^n\\boldsymbol{g}_{t}^i\\\\ \u0026= \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{b_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{j=1}^{b_{s}}\\mathcal{L}(\\mathbf{x}_{i}^j, \\boldsymbol{\\theta}_{t}) \\\\ \u0026= \\frac{1}{nb_{s}}\\sum_{i=1}^n\\sum_{j=1}^{b_{s}}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\mathbf{x}_{i}^j, \\boldsymbol{\\theta}_{t}) \\\\ \u0026= \\frac{1}{B_{s}}\\nabla_{\\boldsymbol{\\theta}}\\sum_{i=1}^{B_{s}}\\mathcal{L}(\\mathbf{x}^i, \\boldsymbol{\\theta}_{t}) \\end{align} $$ 标签平滑 标签平滑是分类任务重的一种正则化技术，其思想是将原来的真实标签(one hot编码)转换为“软化”标签。在正确类别上赋予$ 1 - \\epsilon $的概率，再在其他类别上均匀分配$ \\epsilon $的概率，其中$ K $为多分类的类别总数。\n其中标签分类分布如下\n$$ p(y\\mid\\mathbf{z})=\\mathrm{softmax}_{y}(\\mathbf{z})=\\frac{\\exp[z_{y}]}{\\sum_{y'=1}^{K}\\exp[z_{y'}]} $$ 取负$ \\log $之后得到损失函数\n$$ -\\log p(y\\mid\\mathbf{z})=-\\log[\\mathrm{softmax}_{y}(\\mathbf{z})]=-\\log\\left[\\frac{\\exp[z_{y}]}{\\sum_{y'=1}^{K}\\exp[z_{y'}]}\\right] $$ $$ -\\nabla_{\\mathbf{z}}\\log[\\mathrm{softmax}_{y}(\\mathbf{z})] = \\mathrm{softmax}(\\mathbf{z}) - \\mathbf{e}_{y} $$ 上述损失函数求得梯度之后得到的结果表明了我们最终优化的结果应该使得$ \\mathrm{softmax}(\\mathbf{z}) $和真实标签分布$ \\mathbf{e}_{y} $尽可能接近。\n如果是平滑之后的标签模型输出分类分布和真实标签分类分布如下\n$$ p(y=k\\mid\\mathbf{z})=\\mathrm{softmax}_{k}(\\mathbf{z})=\\frac{\\exp[z_{k}]}{\\sum_{k'=1}^{K}\\exp[z_{k'}]} $$ $$ p(y=k) = q_{k} $$ 两个分布的交叉熵如下，最小化交叉熵等价于最小化两个分布的$ \\mathrm{KL} $散度。\n$$ H(p(y=k), p(y=k\\mid\\mathbf{z})) = -\\sum_{k=1}^K q_{k}\\log\\left[\\frac{\\exp[z_{k}]}{\\sum_{k'=1}^{K}\\exp[z_{k'}]}\\right] $$ $$ \\nabla_{\\mathbf{z}}H(p(y=k), p(y=k\\mid\\mathbf{z})) = \\mathrm{softmax}(\\mathbf{z}) - \\mathbf{y} $$ 平滑标签之后的梯度使得$ \\mathrm{softmax}(\\mathbf{z}) $和平滑标签分布$ \\mathbf{y} $尽可能接近。\n指数移动平均 在优化器一节中我们已经介绍了移动平均的思想使得学习率的收缩与梯度较近的历史信息相关。而模型的指数移动平均主要是进行权重的指数移动平均，使得最终得到的权重参数更加平稳，从而提升泛化能力。\n$$ \\mathbf{ema}_{t+1} = \\rho \\cdot\\mathbf{ema}_{t} + (1 - \\rho)\\cdot \\mathbf{w}_{t} $$ $ \\mathbf{w}_{t} $表示第$ t $步时的模型权重，$ \\mathbf{ema}_{0}=\\mathbf{0} $表示初始指数移动平均权重。\n还有一些小$ \\mathrm{trick} $包括梯度裁剪（$ \\tilde{\\boldsymbol{g}}_{t}=\\frac{\\mathrm{n_{max}}}{\\max(\\lvert \\lvert \\boldsymbol{g}_{t} \\rvert \\rvert_{p}^p, \\mathrm{n_{max}})}\\boldsymbol{g}_{t} $）、早停策略以及混合精度计算（AMP）也有时也都可以优化模型的训练过程。\n","wordCount":"1128","inLanguage":"en","image":"https://fordelkon.github.io/img/optimizers.gif","datePublished":"2025-09-01T10:36:33+09:00","dateModified":"2025-09-01T10:36:33+09:00","author":{"@type":"Person","name":"DL Kong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://fordelkon.github.io/posts/train_trick/"},"publisher":{"@type":"Organization","name":"DL Kong","logo":{"@type":"ImageObject","url":"https://fordelkon.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fordelkon.github.io/ accesskey=h title="DL Kong (Alt + H)"><img src=https://fordelkon.github.io/logo_filled_outlined_6.png alt="Site icon in header" aria-label=logo height=35>DL Kong</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button">
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://fordelkon.github.io/about/ title=About><span>About</span></a></li><li><a href=https://fordelkon.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://fordelkon.github.io/teaching/ title=Teaching><span>Teaching</span></a></li><li><a href=https://fordelkon.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fordelkon.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://fordelkon.github.io/posts/>Posts</a></div><h1 class=post-title>深度学习训练技巧</h1><div class=post-meta><span title='2025-09-01 10:36:33 +0900 +0900'>August 9, 1033</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;DL Kong</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#优化器>优化器</a><ul><li><a href=#sgd优化器>SGD优化器</a></li><li><a href=#使用nesterov动量的sgd优化器>使用Nesterov动量的SGD优化器</a></li><li><a href=#rmsprop优化器>RMSProp优化器</a></li><li><a href=#使用nesterov动量的rmsprop优化器>使用Nesterov动量的RMSProp优化器</a></li><li><a href=#adam优化器>Adam优化器</a></li><li><a href=#使用nesterov动量的adam优化器>使用Nesterov动量的Adam优化器</a></li><li><a href=#adamax优化器>Adamax优化器</a></li><li><a href=#adamw优化器>AdamW优化器</a></li><li><a href=#radam优化器>RAdam优化器</a></li></ul></li><li><a href=#调度器>调度器</a><ul><li><a href=#余弦调度器>余弦调度器</a></li><li><a href=#线性调度器>线性调度器</a></li><li><a href=#多项式调度器>多项式调度器</a></li><li><a href=#预热调度器>预热调度器</a></li></ul></li><li><a href=#梯度累积优化>梯度累积优化</a></li><li><a href=#标签平滑>标签平滑</a></li><li><a href=#指数移动平均>指数移动平均</a></li></ul></nav></div></details></div><div class=post-content><p>进行深度学习实践的过程中我们所需要做的不仅仅只是搭建好模型，如何通过一系列的技巧来训练出一个较为优秀的模型也是至关重要的。没有训练好的模型作为证明就算模型架构再好也无法说明提出的模型的优越性，这一篇博客主要介绍我们可以从哪些方面来优化模型的训练过程。</p><h2 id=优化器>优化器<a hidden class=anchor aria-hidden=true href=#优化器>#</a></h2><p>优化器是模型训练过程中一个很重要的成分，使用不同种类的优化器有时会对训练的过程产生很大的影响，同时优化器中一些超参数的选择有时候也会对训练过程产生很大的影响。下面介绍一些常用的优化器以及其中的超参数介绍：</p><p>介绍优化器之前，我们要先定义训练集<code>$ \mathcal{D}_{train} $</code></p><div>$$
\mathcal{D}_{train} =
\begin{cases}
\lbrace \mathbf{x_{i}}\rbrace_{i=1}^{N_{train}} & no\:labels \\
\lbrace (\mathbf{x}_{i}, y_{i})\rbrace_{i=1}^{N_{train}} & labels
\end{cases}
$$</div><p>然后从训练集中采集包含<code>$ B_{s} $</code>个样本的小批量<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>，下面在上面的设定下介绍各个优化器，为了适应有标签和无标签的情况最终所计算的损失我们统一记为<code>$ \mathcal{L}(\mathbf{x}, \boldsymbol{\theta}) $</code>。</p><h3 id=sgd优化器>SGD优化器<a hidden class=anchor aria-hidden=true href=#sgd优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{\theta}_{t + 1} = \boldsymbol{\theta}_{t} - \eta\: \boldsymbol{g}_{t}\quad &\text{(update the parameters)}
\end{array}
$$</div><p>上述公式中<code>$ \eta $</code>表示学习率，<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。</p><h3 id=使用nesterov动量的sgd优化器>使用Nesterov动量的SGD优化器<a hidden class=anchor aria-hidden=true href=#使用nesterov动量的sgd优化器>#</a></h3><div>$$
\begin{array}{l}
\textcolor{red}{\tilde{\boldsymbol{\theta}}_{t}} = \boldsymbol{\theta}_{t} + \alpha\textcolor{blue}{\boldsymbol{v}_{t}} \quad &\text{(look ahead to the future position)} \\
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \textcolor{red}{\tilde{\boldsymbol{\theta}}_{t}})\quad &\text{(compute gradient at the lookahead position)} \\
\textcolor{blue}{\boldsymbol{v}_{t+1}} = \alpha\textcolor{blue}{\boldsymbol{v}_{t}} - \eta\boldsymbol{g}_{t} \quad &\text{(update the velocity)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} + \textcolor{blue}{\boldsymbol{v}_{t+1}}\quad &\text{(update the parameters)}
\end{array}
$$</div><p>上述公式中<code>$ \eta $</code>表示学习率，<code>$ \alpha $</code>表示动量参数，<code>$ \boldsymbol{v}_{0}=\mathbf{0} $</code>表示初始速度。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。</p><h3 id=rmsprop优化器>RMSProp优化器<a hidden class=anchor aria-hidden=true href=#rmsprop优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{r}_{t+1} = \rho\boldsymbol{r}_{t} + (1 - \rho)\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t} \quad &\text{(update the running average of squared gradients)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \frac{\eta}{\sqrt{ \delta + \boldsymbol{r}_{t+1} }}\circ \boldsymbol{g}_{t} \quad &\text{(update the parameters with the adaptive scaling)}
\end{array}
$$</div><p>上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho $</code>表示衰减速率，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。</p><h3 id=使用nesterov动量的rmsprop优化器>使用Nesterov动量的RMSProp优化器<a hidden class=anchor aria-hidden=true href=#使用nesterov动量的rmsprop优化器>#</a></h3><div>$$
\begin{array}{l}
\textcolor{red}{\tilde{\boldsymbol{\theta}}_{t}} = \boldsymbol{\theta}_{t} + \alpha\textcolor{blue}{\boldsymbol{v}_{t}} \quad &\text{(look ahead to the future position)} \\
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \textcolor{red}{\tilde{\boldsymbol{\theta}}_{t}})\quad &\text{(compute gradient at the lookahead position)} \\
\textcolor{purple}{\boldsymbol{r}_{t+1}} = \rho\textcolor{purple}{\boldsymbol{r}_{t}} + (1 - \rho)\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t} \quad &\text{(update the running average of squared gradients)} \\
\textcolor{blue}{\boldsymbol{v}_{t+1}} = \alpha\textcolor{blue}{\boldsymbol{v}_{t}} - \frac{\eta}{\sqrt{ \delta + \textcolor{purple}{\boldsymbol{r}_{t+1}} }}\circ\boldsymbol{g}_{t} \quad &\text{(update the velocity with adaptive scaling)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} + \textcolor{blue}{\boldsymbol{v}_{t+1}}\quad &\text{(update the parameters)}
\end{array}
$$</div><p>上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho $</code>表示衰减速率，<code>$ \alpha $</code>表示动量参数，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量，<code>$ \boldsymbol{v}_{0}=\mathbf{0} $</code>表示初始速度。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。</p><h3 id=adam优化器>Adam优化器<a hidden class=anchor aria-hidden=true href=#adam优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{s}_{t+1} = \rho_{1}\boldsymbol{s}_{t} + (1 - \rho_{1})\boldsymbol{g}_{t}\quad&\text{(update the biased first momentum)} \\
\boldsymbol{r}_{t+1} = \rho_{2}\boldsymbol{r}_{t} + (1 - \rho_{2})\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t}\quad&\text{(update the biased second momentum)} \\
\hat{\boldsymbol{s}}_{t+1}, \hat{\boldsymbol{r}}_{t+1} = \frac{\boldsymbol{s}_{t+1}}{1 - \rho_{1}^{t + 1}}, \frac{\boldsymbol{r}_{t+1}}{1 - \rho_{2}^{t + 1}}\quad&\text{(bias correction of first momentum and second momentum)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta \:\cdot\frac{\hat{\boldsymbol{s}}_{t+1}}{\delta + \sqrt{ \hat{\boldsymbol{r}}_{t+1} }} \quad&\text{(update the parameters)}
\end{array}
$$</div><p>上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho_{1} $</code>和<code>$ \rho_{2} $</code>表示一阶矩估计和二阶矩估计的衰减速率，<code>$ \boldsymbol{s}_{0}=\mathbf{0} $</code>表示初始累积梯度变量，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。</p><h3 id=使用nesterov动量的adam优化器>使用Nesterov动量的Adam优化器<a hidden class=anchor aria-hidden=true href=#使用nesterov动量的adam优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{s}_{t+1} = \rho_{1}\boldsymbol{s}_{t} + (1 - \rho_{1})\boldsymbol{g}_{t}\quad&\text{(update the biased first momentum)} \\
\boldsymbol{r}_{t+1} = \rho_{2}\boldsymbol{r}_{t} + (1 - \rho_{2})\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t}\quad&\text{(update the biased second momentum)} \\
\hat{\boldsymbol{s}}_{t+1}, \hat{\boldsymbol{r}}_{t+1} = \frac{\boldsymbol{s}_{t+1}}{1 - \rho_{1}^{t + 1}}, \frac{\boldsymbol{r}_{t+1}}{1 - \rho_{2}^{t + 1}}\quad&\text{(bias correction of first momentum and second momentum)} \\
\textcolor{red}{\tilde{\boldsymbol{s}}_{t+1}}=\rho_{1}\hat{\boldsymbol{s}}_{t+1}+\frac{1-\rho_{1}}{1-\rho_{1}^{t+1}}\boldsymbol{g}_{t}\quad&\text{(lookahead adjustment of first momentum)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta \:\cdot\frac{\textcolor{red}{\tilde{\boldsymbol{s}}_{t+1}}}{\delta + \sqrt{ \hat{\boldsymbol{r}}_{t+1} }} \quad&\text{(update the parameters)}
\end{array}
$$</div><p>同样上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho_{1} $</code>和<code>$ \rho_{2} $</code>表示一阶矩估计和二阶矩估计的衰减速率，<code>$ \boldsymbol{s}_{0}=\mathbf{0} $</code>表示初始累积梯度变量，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。使用Nesterov动量的Adam优化器与Adam的区别：<font color=#ff0000>提前预知一阶矩动量</font><code>$ \tilde{\boldsymbol{s}}_{t+1} $</code>。</p><h3 id=adamax优化器>Adamax优化器<a hidden class=anchor aria-hidden=true href=#adamax优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{s}_{t+1} = \rho_{1}\boldsymbol{s}_{t} + (1 - \rho_{1})\boldsymbol{g}_{t}\quad&\text{(update the biased first momentum)} \\
\boldsymbol{r}_{t+1} = \max(\rho_{2}\boldsymbol{r}_{t}, \lvert \boldsymbol{g}_{t} \rvert )\quad&\text{(update exponentially weighted infinity norm)} \\
\hat{\boldsymbol{s}}_{t+1} = \frac{\boldsymbol{s}_{t+1}}{1 - \rho_{1}^{t + 1}}\quad&\text{(bias correction of the first momentum)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta \:\cdot\frac{\hat{\boldsymbol{s}}_{t+1}}{\delta + \boldsymbol{r}_{t+1}} \quad&\text{(update the parameters)}
\end{array}
$$</div><p>同样上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho_{1} $</code>和<code>$ \rho_{2} $</code>表示一阶矩估计和无穷范数的衰减速率，<code>$ \boldsymbol{s}_{0}=\mathbf{0} $</code>表示初始累积梯度变量，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度无穷范数变量。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。Adamax优化器和Adam优化器的区别：<font color=#ff0000>使用无穷范数来代替二阶矩估计</font>。</p><h3 id=adamw优化器>AdamW优化器<a hidden class=anchor aria-hidden=true href=#adamw优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{s}_{t+1} = \rho_{1}\boldsymbol{s}_{t} + (1 - \rho_{1})\boldsymbol{g}_{t}\quad&\text{(update the biased first momentum)} \\
\boldsymbol{r}_{t+1} = \rho_{2}\boldsymbol{r}_{t} + (1 - \rho_{2})\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t}\quad&\text{(update the biased second momentum)} \\
\hat{\boldsymbol{s}}_{t+1}, \hat{\boldsymbol{r}}_{t+1} = \frac{\boldsymbol{s}_{t+1}}{1 - \rho_{1}^{t + 1}}, \frac{\boldsymbol{r}_{t+1}}{1 - \rho_{2}^{t + 1}}\quad&\text{(bias correction of first momentum and second momentum)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta \:\cdot\frac{\hat{\boldsymbol{s}}_{t+1}}{\delta + \sqrt{ \hat{\boldsymbol{r}}_{t+1} }}-\eta\:\cdot \lambda\boldsymbol{\theta}_{t} \quad&\text{(separates weight decay from adaptive gradient scaling, ensuring true L2 regularization)}
\end{array}
$$</div><p>同样上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho_{1} $</code>和<code>$ \rho_{2} $</code>表示一阶矩估计和二阶矩估计的衰减速率，<code>$ \lambda $</code>表示<code>$ \text{L2} $</code>正则项系数。<code>$ \boldsymbol{s}_{0}=\mathbf{0} $</code>表示初始累积梯度变量，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。AdamW优化器和Adam优化器的主要区别：<font color=#ff0000>把</font><code>$ \text{L2} $</code><font color=#ff0000>正则化从梯度更新中分离出来，防止正则化不纯粹</font>。</p><h3 id=radam优化器>RAdam优化器<a hidden class=anchor aria-hidden=true href=#radam优化器>#</a></h3><div>$$
\begin{array}{l}
\boldsymbol{g}_{t} = \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})\quad &\text{(compute gradient at the current position)} \\
\boldsymbol{s}_{t+1} = \rho_{1}\boldsymbol{s}_{t} + (1 - \rho_{1})\boldsymbol{g}_{t}\quad&\text{(update the biased first momentum)} \\
\boldsymbol{r}_{t+1} = \rho_{2}\boldsymbol{r}_{t} + (1 - \rho_{2})\boldsymbol{g}_{t}\circ\boldsymbol{g}_{t}\quad&\text{(update the biased second momentum)} \\
\hat{\boldsymbol{s}}_{t+1}, \hat{\boldsymbol{r}}_{t+1} = \frac{\boldsymbol{s}_{t+1}}{1 - \rho_{1}^{t + 1}}, \frac{\boldsymbol{r}_{t+1}}{1 - \rho_{2}^{t + 1}}\quad&\text{(bias correction of first momentum and the second momentum)} \\
\gamma_{t + 1} = \gamma_{\infty} - \frac{2(t+1)\rho_{2}^{t+1}}{1 - \rho_{2}^{t+1}}\quad &\text{(compute the length of the approximated SMA)} \\
\begin{cases}
\psi_{t+1} = \sqrt{ \frac{(\gamma_{t+1} - 4)(\gamma_{t+1} - 2)\gamma_{\infty}}{(\gamma_{\infty} - 4)(\gamma_{\infty} - 2)\gamma_{t+1}} }\quad&\text{(compute the variance rectification term)} \\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta\:\cdot \psi_{t+1}\frac{\hat{\boldsymbol{s}}_{t+1}}{\delta + \sqrt{ \hat{\boldsymbol{r}}_{t+1}} } \quad&\text{(update the parameters with adaptive momentum)}
\end{cases}\quad&(\text{if}\:\gamma_{t+1} > 4) \\
\begin{cases}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_{t} - \eta\:\cdot \hat{\boldsymbol{s}}_{t+1}\quad\quad\quad\quad\quad&\text{(update the parameters with un-adaptive momentum)}
\end{cases}
\quad&\text{(else)}
\end{array}
$$</div><p>同样上述公式中<code>$ \eta $</code>表示学习率，<code>$ \delta $</code>表示一个较小的常数，用于被小数除时的数值稳定，<code>$ \rho_{1} $</code>和<code>$ \rho_{2} $</code>表示一阶矩估计和二阶矩估计的衰减速率。<code>$ \boldsymbol{s}_{0}=\mathbf{0} $</code>表示初始累积梯度变量，<code>$ \boldsymbol{r}_{0}=\mathbf{0} $</code>表示初始累积梯度平方变量，<code>$ \gamma_{\infty}=\frac{2}{1-\rho_{2}} - 1 $</code>表示计算的近似SMA的最大长度。<code>$ \boldsymbol{\theta}_{t} $</code>表示第<code>$ t $</code>步时的模型参数，<code>$ \boldsymbol{\theta}_{t+1} $</code>表示在处理过小批量数据<code>$ \lbrace\mathbf{x}^{1}, \mathbf{x}^2, \dots, \mathbf{x}^{B_{s}}\rbrace $</code>之后第<code>$ t+1 $</code>步的模型参数。其与Adam优化器的区别是)，<font color=#ff0000>RAdam提出了一个方差校正因子</font><code>$ \psi $</code>(rectification term)，<font color=#ff0000>自动调整有效学习率，让优化在初期更加稳定。</font></p><h2 id=调度器>调度器<a hidden class=anchor aria-hidden=true href=#调度器>#</a></h2><p>上面一节中优化器中的超参数学习率<code>$ \eta $</code>我们设定成了固定值，但是在真正实践中我们可以设定学习率<code>$ \eta $</code>随着<code>$ \text{epoch} $</code>变化(<code>$ \text{epoch-based} $</code>)或者随着<code>$ \text{step} $</code>变化(<code>$ \text{step-based} $</code>)，这些变化我们可以通过调度器来进行实现，下面我们介绍一些常见的调度器。</p><h3 id=余弦调度器>余弦调度器<a hidden class=anchor aria-hidden=true href=#余弦调度器>#</a></h3><center><img src=./img/cos_step_epoch.png alt=图片1 width=400></center>由图可知，`$ \text{step-based} $`的余弦调度器在每个`$ \text{step} $`都会发生变化，而`$ \text{epoch-based} $`的余弦调度器在每个`$ \text{epoch} $`发生变化，而在相邻之间的`$ \text{step} $`区间不会发生变化，通常`$ \text{1 epoch}=\text{1 step}\times \text{num batches} $`。<div>$$
\eta_{t} = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left( 1 + \cos\left( \frac{t}{T_{\text{max}}} \pi\right) \right)
$$</div><p>其中<code>$ t $</code>表示调度的粒度，既可以是<code>$ \text{step} $</code>级别又可以是<code>$ \text{epoch} $</code>级别。</p><h3 id=线性调度器>线性调度器<a hidden class=anchor aria-hidden=true href=#线性调度器>#</a></h3><center><img src=./img/linear_step_epoch.png alt=图片1 width=400></center>同余弦调度器的绘制说明，线性调度器的公式为<div>$$
\eta_{t} = \eta_{\text{max}} - \frac{t}{T_{\text{max}}}(\eta_{\text{max}} - \eta_{\text{min}})
$$</div><p>其中<code>$ t $</code>表示调度的粒度，既可以是<code>$ \text{step} $</code>级别又可以是<code>$ \text{epoch} $</code>级别。</p><h3 id=多项式调度器>多项式调度器<a hidden class=anchor aria-hidden=true href=#多项式调度器>#</a></h3><center><img src=./img/poly_step_epoch.png alt=图片1 width=400></center>同余弦调度器绘制说明，多项式调度器的公式为<div>$$
\eta_{t} = \eta_{\text{min}} + (\eta_{\text{max}} - \eta_{\text{min}})\left( 1 - \frac{t}{T_{\text{max}}} \right)^p
$$</div><p>其中<code>$ t $</code>表示调度的粒度，既可以是<code>$ \text{step} $</code>级别又可以是<code>$ \text{epoch} $</code>级别。<code>$ p $</code>表示多项式幂指数。</p><h3 id=预热调度器>预热调度器<a hidden class=anchor aria-hidden=true href=#预热调度器>#</a></h3><center><img src=./img/warmup.png alt=图片1 width=400></center>预热调度器一般是使用`$ \text{step} $`粒度级别的线性增长调度。也就是上图中的5个`$ \text{epoch} $`之前的过程都属于`$ \text{warmup} $`过程。<h2 id=梯度累积优化>梯度累积优化<a hidden class=anchor aria-hidden=true href=#梯度累积优化>#</a></h2><p>当使用单卡GPU进行模型训练时当我们的批次大小设定过大时会出现显存不足的情况，所以单卡GPU往往设定的批次大小偏小以避免显存爆炸，但是小批量的训练方式往往会导致多个批次计算的梯度不稳定，最终导致损失的收敛不稳定。为了解决上述困境，一个有效的解决方法就是使用<font color=#ff0000>梯度累积优化</font>。</p><p>假设小批量设定为<code>$ b_{s} $</code>，累积的大批量设定为<code>$ B_{s} $</code>，习惯上<code>$ B_{s}=n\:\cdot b_{s} $</code>，<code>$ n $</code>表示梯度累积的<code>$ \text{step} $</code>数。我们可以证明<code>$ n $</code>个<code>$ b_{s} $</code>批次梯度累积优化等价于一次<code>$ B_{s} $</code>批次梯度优化。</p><div>$$
\begin{align}
\boldsymbol{g}_{t} &= \frac{1}{n}\sum_{i=1}^n\boldsymbol{g}_{t}^i\\
&= \frac{1}{n}\sum_{i=1}^n \frac{1}{b_{s}}\nabla_{\boldsymbol{\theta}}\sum_{j=1}^{b_{s}}\mathcal{L}(\mathbf{x}_{i}^j, \boldsymbol{\theta}_{t}) \\
&= \frac{1}{nb_{s}}\sum_{i=1}^n\sum_{j=1}^{b_{s}}\nabla_{\boldsymbol{\theta}}\mathcal{L}(\mathbf{x}_{i}^j, \boldsymbol{\theta}_{t}) \\
&= \frac{1}{B_{s}}\nabla_{\boldsymbol{\theta}}\sum_{i=1}^{B_{s}}\mathcal{L}(\mathbf{x}^i, \boldsymbol{\theta}_{t})
\end{align}
$$</div><h2 id=标签平滑>标签平滑<a hidden class=anchor aria-hidden=true href=#标签平滑>#</a></h2><p>标签平滑是分类任务重的一种正则化技术，其思想是将原来的真实标签(one hot编码)转换为“软化”标签。在正确类别上赋予<code>$ 1 - \epsilon $</code>的概率，再在其他类别上均匀分配<code>$ \epsilon $</code>的概率，其中<code>$ K $</code>为多分类的类别总数。</p><p>其中标签分类分布如下</p><div>$$
p(y\mid\mathbf{z})=\mathrm{softmax}_{y}(\mathbf{z})=\frac{\exp[z_{y}]}{\sum_{y'=1}^{K}\exp[z_{y'}]}
$$</div><p>取负<code>$ \log $</code>之后得到损失函数</p><div>$$
-\log p(y\mid\mathbf{z})=-\log[\mathrm{softmax}_{y}(\mathbf{z})]=-\log\left[\frac{\exp[z_{y}]}{\sum_{y'=1}^{K}\exp[z_{y'}]}\right]
$$</div><div>$$
-\nabla_{\mathbf{z}}\log[\mathrm{softmax}_{y}(\mathbf{z})] = \mathrm{softmax}(\mathbf{z}) - \mathbf{e}_{y}
$$</div><p>上述损失函数求得梯度之后得到的结果表明了我们最终优化的结果应该使得<code>$ \mathrm{softmax}(\mathbf{z}) $</code>和真实标签分布<code>$ \mathbf{e}_{y} $</code>尽可能接近。</p><p>如果是平滑之后的标签模型输出分类分布和真实标签分类分布如下</p><div>$$
p(y=k\mid\mathbf{z})=\mathrm{softmax}_{k}(\mathbf{z})=\frac{\exp[z_{k}]}{\sum_{k'=1}^{K}\exp[z_{k'}]}
$$</div><div>$$
p(y=k) = q_{k}
$$</div><p>两个分布的交叉熵如下，最小化交叉熵等价于最小化两个分布的<code>$ \mathrm{KL} $</code>散度。</p><div>$$
H(p(y=k), p(y=k\mid\mathbf{z})) = -\sum_{k=1}^K q_{k}\log\left[\frac{\exp[z_{k}]}{\sum_{k'=1}^{K}\exp[z_{k'}]}\right]
$$</div><div>$$
\nabla_{\mathbf{z}}H(p(y=k), p(y=k\mid\mathbf{z})) = \mathrm{softmax}(\mathbf{z}) - \mathbf{y}
$$</div><p>平滑标签之后的梯度使得<code>$ \mathrm{softmax}(\mathbf{z}) $</code>和平滑标签分布<code>$ \mathbf{y} $</code>尽可能接近。</p><h2 id=指数移动平均>指数移动平均<a hidden class=anchor aria-hidden=true href=#指数移动平均>#</a></h2><p>在优化器一节中我们已经介绍了移动平均的思想使得学习率的收缩与梯度较近的历史信息相关。而模型的指数移动平均主要是进行权重的指数移动平均，使得最终得到的权重参数更加平稳，从而提升泛化能力。</p><div>$$
\mathbf{ema}_{t+1} = \rho \cdot\mathbf{ema}_{t} + (1 - \rho)\cdot \mathbf{w}_{t}
$$</div><p><code>$ \mathbf{w}_{t} $</code>表示第<code>$ t $</code>步时的模型权重，<code>$ \mathbf{ema}_{0}=\mathbf{0} $</code>表示初始指数移动平均权重。</p><p>还有一些小<code>$ \mathrm{trick} $</code>包括梯度裁剪（<code>$ \tilde{\boldsymbol{g}}_{t}=\frac{\mathrm{n_{max}}}{\max(\lvert \lvert \boldsymbol{g}_{t} \rvert \rvert_{p}^p, \mathrm{n_{max}})}\boldsymbol{g}_{t} $</code>）、早停策略以及混合精度计算（AMP）也有时也都可以优化模型的训练过程。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://fordelkon.github.io/tags/dl/>DL</a></li><li><a href=https://fordelkon.github.io/tags/python/>Python</a></li></ul><nav class=paginav><a class=next href=https://fordelkon.github.io/posts/docker_intro/><span class=title>Next »</span><br><span>使用Docker的艺术</span></a></nav></footer><div class=giscus_comments><script src=https://giscus.app/client.js data-repo=jesse-wei/jessewei.dev-PaperMod data-repo-id=R_kgDOJhJgPg data-category=Comments data-category-id=DIC_kwDOJhJgPs4CWei3 data-mapping=pathname data-strict=1 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></div><script async>document.querySelector("div.giscus_comments > script").setAttribute("data-theme",localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme"):window.matchMedia("(prefers-color-scheme: dark)").matches?"transparent_dark":"light"),document.querySelector("#theme-toggle").addEventListener("click",()=>{let e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:{setConfig:{theme:localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme")==="dark"?"light":"transparent_dark":document.body.className.includes("dark")?"light":"transparent_dark"}}},"https://giscus.app")})</script></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/fordelkon rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://x.com/fordelkon rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=mailto:dlkong201893@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://fordelkon.github.io/>DL Kong</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>